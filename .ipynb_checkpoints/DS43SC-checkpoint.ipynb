{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y6SKlgYrpcym"
   },
   "source": [
    "# Neural Networks Sprint Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BrEbRrjVphPM"
   },
   "source": [
    "## 1) Define the following terms:\n",
    "\n",
    "- Neuron\n",
    "- Input Layer\n",
    "- Hidden Layer\n",
    "- Output Layer\n",
    "- Activation\n",
    "- Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q5EksLqnp4oB"
   },
   "source": [
    " ## YOUR ANSWER HERE\n",
    "    \n",
    "### NEURON - The structure of brain cells that a neural network is trying to emulate into computer algorithms which can make predictions.\n",
    "\n",
    "### INPUT LAYER - The input layer is what receives input from the dataset, the visible layer, the only layer that interacts directly with our dataset.\n",
    "\n",
    "### HIDDEN LAYER - All the layers after the input layer and before the output layer.  It's hidden in that we don't get to directly interact with this layer.\n",
    "\n",
    "### OUTPUT LAYER - The final layer of the neural network that outputs a vector of values suitable for the problem we're trying to solve.  The output layer in a classificaiton problem has been modified by an activation function (i.e. sigmoid / Relu).\n",
    "\n",
    "### ACTIVATION - A decision function that decides how much signal to pass onto the next layer.  They can be like the step function where everything equal to or greater than 0 is assigned a 1, otherwise -1.\n",
    "\n",
    "### BACKPROPAGATION - a neural network algorithm where the weights in the weighted sum are first calculated forwards in the manner of feed forward algorithm, and then backwards starting from the output layer using paritial derivatives or the Chain Rule of calculus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ri_gRA2Jp728"
   },
   "source": [
    "## 2) Create a perceptron class that can model the behavior of an AND gate. You can use the following table as your training data:\n",
    "\n",
    "| x1 | x2 | x3 | y |\n",
    "|----|----|----|---|\n",
    "| 1  | 1  | 1  | 1 |\n",
    "| 1  | 0  | 1  | 0 |\n",
    "| 0  | 1  | 1  | 0 |\n",
    "| 0  | 0  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ig6ZTH8tpQ19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.25091976],\n",
       "       [ 0.90142861],\n",
       "       [ 0.46398788]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Your Code Here #####\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "inputs = np.array([[1, 1, 1],\n",
    "                  [1, 0, 1],\n",
    "                  [0, 1, 1],\n",
    "                  [0, 0, 1]])\n",
    "correct_outputs = [[1],\n",
    "                  [0],\n",
    "                  [0],\n",
    "                  [0]]\n",
    "\n",
    "# sigmoid activation function:\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# initialize some random weights\n",
    "weights = 2 * np.random.random((3, 1)) - 1\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.11449673],\n",
       "       [0.21306812],\n",
       "       [1.3654165 ],\n",
       "       [0.46398788]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate weighted sum of inputs and weights\n",
    "weighted_sum = np.dot(inputs, weights)\n",
    "weighted_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.75296649],\n",
       "       [0.55306642],\n",
       "       [0.79663861],\n",
       "       [0.61395979]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output the activated value for the end of 1 training epoch\n",
    "activated_output = sigmoid(weighted_sum)\n",
    "activated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24703351],\n",
       "       [-0.55306642],\n",
       "       [-0.79663861],\n",
       "       [-0.61395979]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Error for our first pass\n",
    "error = correct_outputs - activated_output\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05377007],\n",
       "       [-0.12820984],\n",
       "       [-0.17062609],\n",
       "       [-0.13988803]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we take our errors and update the weights accordingly\n",
    "# using derivative of the sigmoid function to calculate weight updates\n",
    "adjustments = error * sigmoid_derivative(activated_output)\n",
    "adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.32535954],\n",
       "       [ 0.78457259],\n",
       "       [ 0.07903399]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update weights\n",
    "weights += np.dot(inputs.T, adjustments)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized weights after training: \n",
      "[[ 11.83991275]\n",
      " [ 11.83991275]\n",
      " [-18.04817835]]\n",
      "Output After Training:\n",
      "[[9.96429763e-01]\n",
      " [2.00888375e-03]\n",
      " [2.00888375e-03]\n",
      " [1.45179935e-08]]\n"
     ]
    }
   ],
   "source": [
    "# Putting it all together:\n",
    "for iteration in range(10000):\n",
    "  # Weighted sum of inputs and weights\n",
    "  weighted_sum = np.dot(inputs, weights)\n",
    "  # Activate with sigmoid function\n",
    "  activated_output = sigmoid(weighted_sum)\n",
    "  # Calculate Error\n",
    "  error = correct_outputs - activated_output\n",
    "  # Calculate weight adjustments with sigmoid_derivative\n",
    "  adjustments = error * sigmoid_derivative(activated_output)\n",
    "  # Update weights\n",
    "  weights += np.dot(inputs.T, adjustments)\n",
    "    \n",
    "print('optimized weights after training: ')\n",
    "print(weights)\n",
    "\n",
    "print(\"Output After Training:\")\n",
    "print(activated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86HyRi8Osr3U"
   },
   "source": [
    "## 3) Implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. \n",
    "- Your network must have one hidden layer. \n",
    "- You do not have to update weights via gradient descent. You can use something like the derivative of the sigmoid function to update weights.\n",
    "- Train your model on the Heart Disease dataset from UCI:\n",
    "\n",
    "[Github Dataset](https://github.com/ryanleeallred/datasets/blob/master/heart.csv)\n",
    "\n",
    "[Raw File on Github](https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CNfiajv3v4Ed"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Your Code Here #####\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>112</td>\n",
       "      <td>230</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>134</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>197</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>221</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>163</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>197</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>177</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   58    1   2       112   230    0        0      165      0      2.5      1   \n",
       "1   59    1   3       134   204    0        1      162      0      0.8      2   \n",
       "2   58    0   0       130   197    0        1      131      0      0.6      1   \n",
       "3   56    1   1       130   221    0        0      163      0      0.0      2   \n",
       "4   44    1   0       110   197    0        0      177      0      0.0      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   1     3       0  \n",
       "1   2     2       0  \n",
       "2   0     2       1  \n",
       "3   0     3       1  \n",
       "4   1     2       0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create X and y arrays\n",
    "y = df['target'].values.astype('float')\n",
    "X = df.drop('target', axis=1)\n",
    "X = X.values.astype('float')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "class Neural_Network(object):\n",
    "    def __init__(self):\n",
    "        self.inputs = 13\n",
    "        self.hiddenNodes = 4\n",
    "        self.outputNodes = 1\n",
    "        \n",
    "        # Initialize Weights\n",
    "        self.L1_weights = np.random.randn(self.inputs, self.hiddenNodes) # 3x4\n",
    "        self.L2_weights = np.random.randn(self.hiddenNodes, self.outputNodes) # 4x1\n",
    "        \n",
    "    def feed_forward(self, X):\n",
    "        # Weighted sum between input and hidden layer\n",
    "        self.hidden_sum = np.dot(X, self.L1_weights)\n",
    "        # Activations of weighted sum\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        # Weighted sum between hidden and output\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.L2_weights)\n",
    "        # final activation of output\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        return self.activated_output\n",
    "    \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1 + np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def backward(self, X, y, o):\n",
    "        # backward propagate through the network\n",
    "        self.o_error = y - o # error in output\n",
    "        self.o_delta = self.o_error*self.sigmoidPrime(o) # applying derivative of sigmoid to output error\n",
    "        \n",
    "        self.z2_error = self.o_delta.dot(self.L2_weights.T) # z2 errro: how much of our hidden layer weights contributed to output error\n",
    "        self.z2_delta = self.z2_error*self.sigmoidPrime(self.activated_hidden) # applying derivative of sigmoid to z2_error\n",
    "        \n",
    "        self.L1_weights += X.T.dot(self.z2_delta) # adjusting first set (input --> hidden) weights\n",
    "        self.L2_weights += self.activated_hidden.T.dot(self.o_delta) # adjusting second set (hidden --> output) weights\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X, y, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------- EPOCH 1 ------------+\n",
      "Input: \n",
      " [[58.  1.  2. ...  1.  1.  3.]\n",
      " [59.  1.  3. ...  2.  2.  2.]\n",
      " [58.  0.  0. ...  1.  0.  2.]\n",
      " ...\n",
      " [39.  0.  2. ...  1.  0.  2.]\n",
      " [43.  1.  0. ...  1.  4.  3.]\n",
      " [52.  1.  0. ...  2.  2.  3.]]\n",
      "Actual Ouput: \n",
      " [0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1.\n",
      " 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0.\n",
      " 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1.\n",
      " 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1.\n",
      " 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0.\n",
      " 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0.\n",
      " 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1.\n",
      " 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0.\n",
      " 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0.]\n",
      "Predicted Output: \n",
      "[[0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186309]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44154321]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.43388571]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.4416975 ]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.43392015]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.43383568]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]\n",
      " [0.44186315]]\n",
      "Loss: \n",
      "0.2585775976283036\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (303,303) and (1,4) not aligned: 303 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-fe0137129c29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss: \\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# MSE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Normalizing the data and shuffling it did not lower my Loss value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-ca25d9c51491>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-ca25d9c51491>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, X, y, o)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo_delta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo_error\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoidPrime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# applying derivative of sigmoid to output error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz2_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo_delta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL2_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# z2 errro: how much of our hidden layer weights contributed to output error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz2_delta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz2_error\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoidPrime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivated_hidden\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# applying derivative of sigmoid to z2_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (303,303) and (1,4) not aligned: 303 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "NN = Neural_Network()\n",
    "\n",
    "for i in range(10000):\n",
    "    if i+1 in [1, 2, 3, 4, 5] or (i+1) % 50 == 0:\n",
    "        print('+--------- EPOCH', i+1, '------------+')\n",
    "        print(\"Input: \\n\", X)\n",
    "        print(\"Actual Ouput: \\n\", y)\n",
    "        print(\"Predicted Output: \\n\" + str(NN.feed_forward(X)))\n",
    "        print(\"Loss: \\n\" + str(np.mean(np.square(y - NN.feed_forward(X))))) # MSE\n",
    "        print(\"\\n\")\n",
    "    NN.train(X, y)\n",
    "    \n",
    "# Normalizing the data and shuffling it did not lower my Loss value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GGT1oRzXw3H9"
   },
   "source": [
    "## 4) Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy. \n",
    "\n",
    "- Use the Heart Disease Dataset (binary classification)\n",
    "- Use an appropriate loss function for a binary classification task\n",
    "- Use an appropriate activation function on the final layer of your network. \n",
    "- Train your model using verbose output for ease of grading.\n",
    "- Use GridSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "- When hyperparameter tuning, show you work by adding code cells for each new experiment. \n",
    "- Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "- You must hyperparameter tune at least 5 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XWw4IYxLxKwH"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Your Code Here #####\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[63.,  1.,  3., ...,  0.,  0.,  1.],\n",
       "       [37.,  1.,  2., ...,  0.,  0.,  2.],\n",
       "       [41.,  0.,  1., ...,  2.,  0.,  2.],\n",
       "       ...,\n",
       "       [68.,  1.,  0., ...,  1.,  2.,  3.],\n",
       "       [57.,  1.,  0., ...,  1.,  1.,  3.],\n",
       "       [57.,  0.,  1., ...,  1.,  1.,  2.]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy = df.copy()\n",
    "target = df_copy.pop('target')\n",
    "y = target.values\n",
    "\n",
    "X = df_copy.values\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First I build and run Cross-Validation on a KerasClassifier without any hyperparameter tuning to get a \n",
    "### baseline evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "K-Fold Cross-Validation results -> Mean: 0.81, Standard deviation: 0.02\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# define 5-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "inputs = X.shape[1]\n",
    "epochs = 100\n",
    "batch_size = 10\n",
    "\n",
    "# baseline model generator for use with the KerasClassifier wrapper for scikit learn\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=inputs, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# evaluate model with normalized dataset using a Pipeline\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=baseline_model, epochs=epochs, batch_size=batch_size, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "results = cross_val_score(pipeline, X, y, cv=kfold)\n",
    "print()\n",
    "print()\n",
    "print(\"K-Fold Cross-Validation results -> Mean: {:.2f}, Standard deviation: {:.2f}\".format(results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning: Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sammylee/miniconda3/envs/datascience/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n",
      "\n",
      "Best: 0.83 using {'kerasclassifier__batch_size': 10, 'kerasclassifier__epochs': 20}\n",
      "\n",
      "Means: 0.834983493235245, Stdev: 0.03660535740505067 with: {'kerasclassifier__batch_size': 10, 'kerasclassifier__epochs': 20}\n",
      "Means: 0.8085808604463885, Stdev: 0.03705298353831973 with: {'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 20}\n",
      "Means: 0.7326732763756226, Stdev: 0.03368473675270844 with: {'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 20}\n",
      "Means: 0.7590759099513391, Stdev: 0.028848009359369646 with: {'kerasclassifier__batch_size': 60, 'kerasclassifier__epochs': 20}\n",
      "Means: 0.66996699237194, Stdev: 0.062072692809988624 with: {'kerasclassifier__batch_size': 80, 'kerasclassifier__epochs': 20}\n",
      "Means: 0.7326732639825777, Stdev: 0.03887910657844372 with: {'kerasclassifier__batch_size': 100, 'kerasclassifier__epochs': 20}\n"
     ]
    }
   ],
   "source": [
    "pipe = make_pipeline(StandardScaler(), KerasClassifier(build_fn=baseline_model, verbose=3))\n",
    "\n",
    "param_grid = {'kerasclassifier__batch_size': [10, 20, 40, 60, 80, 100],\n",
    "             'kerasclassifier__epochs': [20]}\n",
    "\n",
    "# Create grid search\n",
    "grid = GridSearchCV(estimator=pipe, param_grid=param_grid, cv=kfold, n_jobs=-1)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "# Report results\n",
    "print()\n",
    "print(\"Best: {:.2f} using {}\".format(grid_result.best_score_, grid_result.best_params_))\n",
    "print()\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning: Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sammylee/miniconda3/envs/datascience/lib/python3.6/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/Users/sammylee/miniconda3/envs/datascience/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "Epoch 2/60\n",
      "Epoch 3/60\n",
      "Epoch 4/60\n",
      "Epoch 5/60\n",
      "Epoch 6/60\n",
      "Epoch 7/60\n",
      "Epoch 8/60\n",
      "Epoch 9/60\n",
      "Epoch 10/60\n",
      "Epoch 11/60\n",
      "Epoch 12/60\n",
      "Epoch 13/60\n",
      "Epoch 14/60\n",
      "Epoch 15/60\n",
      "Epoch 16/60\n",
      "Epoch 17/60\n",
      "Epoch 18/60\n",
      "Epoch 19/60\n",
      "Epoch 20/60\n",
      "Epoch 21/60\n",
      "Epoch 22/60\n",
      "Epoch 23/60\n",
      "Epoch 24/60\n",
      "Epoch 25/60\n",
      "Epoch 26/60\n",
      "Epoch 27/60\n",
      "Epoch 28/60\n",
      "Epoch 29/60\n",
      "Epoch 30/60\n",
      "Epoch 31/60\n",
      "Epoch 32/60\n",
      "Epoch 33/60\n",
      "Epoch 34/60\n",
      "Epoch 35/60\n",
      "Epoch 36/60\n",
      "Epoch 37/60\n",
      "Epoch 38/60\n",
      "Epoch 39/60\n",
      "Epoch 40/60\n",
      "Epoch 41/60\n",
      "Epoch 42/60\n",
      "Epoch 43/60\n",
      "Epoch 44/60\n",
      "Epoch 45/60\n",
      "Epoch 46/60\n",
      "Epoch 47/60\n",
      "Epoch 48/60\n",
      "Epoch 49/60\n",
      "Epoch 50/60\n",
      "Epoch 51/60\n",
      "Epoch 52/60\n",
      "Epoch 53/60\n",
      "Epoch 54/60\n",
      "Epoch 55/60\n",
      "Epoch 56/60\n",
      "Epoch 57/60\n",
      "Epoch 58/60\n",
      "Epoch 59/60\n",
      "Epoch 60/60\n",
      "\n",
      "Best: 0.86 using {'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 60}\n",
      "\n",
      "Means: 0.8085808565120886, Stdev: 0.027084999565204528 with: {'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 20}\n",
      "Means: 0.8382838338908583, Stdev: 0.025120703385117696 with: {'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 40}\n",
      "Means: 0.8580858030728381, Stdev: 0.046038626951500045 with: {'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 60}\n",
      "Means: 0.785478547756428, Stdev: 0.0446849114181458 with: {'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 20}\n",
      "Means: 0.7920792006423371, Stdev: 0.03163195199184503 with: {'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 40}\n",
      "Means: 0.8415841590059866, Stdev: 0.03088014321205388 with: {'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 60}\n"
     ]
    }
   ],
   "source": [
    "pipe = make_pipeline(StandardScaler(), KerasClassifier(build_fn=baseline_model, verbose=3))\n",
    "\n",
    "param_grid = {'kerasclassifier__batch_size': [20, 40],\n",
    "             'kerasclassifier__epochs': [20, 40, 60]}\n",
    "\n",
    "# Create grid search\n",
    "grid = GridSearchCV(estimator=pipe, param_grid=param_grid, cv=kfold, n_jobs=-1)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "# Report results\n",
    "print()\n",
    "print(\"Best: {:.2f} using {}\".format(grid_result.best_score_, grid_result.best_params_))\n",
    "print()\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning: Activation in Input Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sammylee/miniconda3/envs/datascience/lib/python3.6/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/Users/sammylee/miniconda3/envs/datascience/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "Epoch 2/60\n",
      "Epoch 3/60\n",
      "Epoch 4/60\n",
      "Epoch 5/60\n",
      "Epoch 6/60\n",
      "Epoch 7/60\n",
      "Epoch 8/60\n",
      "Epoch 9/60\n",
      "Epoch 10/60\n",
      "Epoch 11/60\n",
      "Epoch 12/60\n",
      "Epoch 13/60\n",
      "Epoch 14/60\n",
      "Epoch 15/60\n",
      "Epoch 16/60\n",
      "Epoch 17/60\n",
      "Epoch 18/60\n",
      "Epoch 19/60\n",
      "Epoch 20/60\n",
      "Epoch 21/60\n",
      "Epoch 22/60\n",
      "Epoch 23/60\n",
      "Epoch 24/60\n",
      "Epoch 25/60\n",
      "Epoch 26/60\n",
      "Epoch 27/60\n",
      "Epoch 28/60\n",
      "Epoch 29/60\n",
      "Epoch 30/60\n",
      "Epoch 31/60\n",
      "Epoch 32/60\n",
      "Epoch 33/60\n",
      "Epoch 34/60\n",
      "Epoch 35/60\n",
      "Epoch 36/60\n",
      "Epoch 37/60\n",
      "Epoch 38/60\n",
      "Epoch 39/60\n",
      "Epoch 40/60\n",
      "Epoch 41/60\n",
      "Epoch 42/60\n",
      "Epoch 43/60\n",
      "Epoch 44/60\n",
      "Epoch 45/60\n",
      "Epoch 46/60\n",
      "Epoch 47/60\n",
      "Epoch 48/60\n",
      "Epoch 49/60\n",
      "Epoch 50/60\n",
      "Epoch 51/60\n",
      "Epoch 52/60\n",
      "Epoch 53/60\n",
      "Epoch 54/60\n",
      "Epoch 55/60\n",
      "Epoch 56/60\n",
      "Epoch 57/60\n",
      "Epoch 58/60\n",
      "Epoch 59/60\n",
      "Epoch 60/60\n",
      "\n",
      "Best: 0.85 using {'kerasclassifier__activation': 'linear', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 60}\n",
      "\n",
      "Means: 0.8019801988066619, Stdev: 0.03486255629388665 with: {'kerasclassifier__activation': 'softmax', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 20}\n",
      "Means: 0.8448844915962849, Stdev: 0.03945138477757526 with: {'kerasclassifier__activation': 'softmax', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 40}\n",
      "Means: 0.8382838299565583, Stdev: 0.03261717377893366 with: {'kerasclassifier__activation': 'softmax', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 60}\n",
      "Means: 0.7854785527726604, Stdev: 0.04986362472650458 with: {'kerasclassifier__activation': 'softmax', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 20}\n",
      "Means: 0.8184818491683935, Stdev: 0.04091413505507978 with: {'kerasclassifier__activation': 'softmax', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 40}\n",
      "Means: 0.8250825019559451, Stdev: 0.01921619527906537 with: {'kerasclassifier__activation': 'softmax', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 60}\n",
      "Means: 0.7920792102813721, Stdev: 0.040982715912529624 with: {'kerasclassifier__activation': 'relu', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 20}\n",
      "Means: 0.8184818489716785, Stdev: 0.04114490325439128 with: {'kerasclassifier__activation': 'relu', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 40}\n",
      "Means: 0.834983499136695, Stdev: 0.014832532261552966 with: {'kerasclassifier__activation': 'relu', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 60}\n",
      "Means: 0.8283828396608334, Stdev: 0.029845405011315194 with: {'kerasclassifier__activation': 'relu', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 20}\n",
      "Means: 0.8283828376936834, Stdev: 0.02290513749092333 with: {'kerasclassifier__activation': 'relu', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 40}\n",
      "Means: 0.8217821849061318, Stdev: 0.03615984228033367 with: {'kerasclassifier__activation': 'relu', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 60}\n",
      "Means: 0.825082514545705, Stdev: 0.023231217681530584 with: {'kerasclassifier__activation': 'tanh', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 20}\n",
      "Means: 0.8382838220879583, Stdev: 0.0546971451703094 with: {'kerasclassifier__activation': 'tanh', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 40}\n",
      "Means: 0.825082514545705, Stdev: 0.03689397603414703 with: {'kerasclassifier__activation': 'tanh', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 60}\n",
      "Means: 0.7755775513133594, Stdev: 0.0692796789871742 with: {'kerasclassifier__activation': 'tanh', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 20}\n",
      "Means: 0.8250825068738201, Stdev: 0.0429580475167447 with: {'kerasclassifier__activation': 'tanh', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 40}\n",
      "Means: 0.83498350326771, Stdev: 0.020890654771780114 with: {'kerasclassifier__activation': 'tanh', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 60}\n",
      "Means: 0.8052805249053653, Stdev: 0.04578067341197294 with: {'kerasclassifier__activation': 'sigmoid', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 20}\n",
      "Means: 0.8283828335626684, Stdev: 0.048399714712125445 with: {'kerasclassifier__activation': 'sigmoid', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 40}\n",
      "Means: 0.834983503070995, Stdev: 0.03716761407356337 with: {'kerasclassifier__activation': 'sigmoid', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 60}\n",
      "Means: 0.7590759094841409, Stdev: 0.07822434823264192 with: {'kerasclassifier__activation': 'sigmoid', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 20}\n",
      "Means: 0.8184818560534185, Stdev: 0.03483077110074373 with: {'kerasclassifier__activation': 'sigmoid', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 40}\n",
      "Means: 0.834983496382685, Stdev: 0.04353638392127329 with: {'kerasclassifier__activation': 'sigmoid', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 60}\n",
      "Means: 0.825082510611405, Stdev: 0.017289528873828498 with: {'kerasclassifier__activation': 'hard_sigmoid', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 20}\n",
      "Means: 0.8382838299565583, Stdev: 0.04138991820862421 with: {'kerasclassifier__activation': 'hard_sigmoid', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 40}\n",
      "Means: 0.834983499136695, Stdev: 0.032216040185515465 with: {'kerasclassifier__activation': 'hard_sigmoid', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 60}\n",
      "Means: 0.7788778911329338, Stdev: 0.022116283426281316 with: {'kerasclassifier__activation': 'hard_sigmoid', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 20}\n",
      "Means: 0.8316831724478466, Stdev: 0.03485126839818494 with: {'kerasclassifier__activation': 'hard_sigmoid', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 40}\n",
      "Means: 0.8415841590059866, Stdev: 0.023075434040188757 with: {'kerasclassifier__activation': 'hard_sigmoid', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 60}\n",
      "Means: 0.8448844876619849, Stdev: 0.054599859847323726 with: {'kerasclassifier__activation': 'linear', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 20}\n",
      "Means: 0.8448844837276849, Stdev: 0.04337088772877709 with: {'kerasclassifier__activation': 'linear', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 40}\n",
      "Means: 0.8283828374969684, Stdev: 0.03142026310929361 with: {'kerasclassifier__activation': 'linear', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 60}\n",
      "Means: 0.7722772363782322, Stdev: 0.034616477308995204 with: {'kerasclassifier__activation': 'linear', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 20}\n",
      "Means: 0.8085808645774035, Stdev: 0.03810873424616878 with: {'kerasclassifier__activation': 'linear', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 40}\n",
      "Means: 0.8514851485148515, Stdev: 0.028195173013639517 with: {'kerasclassifier__activation': 'linear', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 60}\n"
     ]
    }
   ],
   "source": [
    "def baseline_model(activation='relu'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=inputs, activation=activation))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), KerasClassifier(build_fn=baseline_model, verbose=3))\n",
    "\n",
    "param_grid = {'kerasclassifier__batch_size': [20, 40],\n",
    "             'kerasclassifier__epochs': [20, 40, 60],\n",
    "             'kerasclassifier__activation': ['softmax', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']}\n",
    "\n",
    "# Create grid search\n",
    "grid = GridSearchCV(estimator=pipe, param_grid=param_grid, cv=kfold, n_jobs=-1)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "# Report results\n",
    "print()\n",
    "print(\"Best: {:.2f} using {}\".format(grid_result.best_score_, grid_result.best_params_))\n",
    "print()\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning: Activation in Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sammylee/miniconda3/envs/datascience/lib/python3.6/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/Users/sammylee/miniconda3/envs/datascience/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "Epoch 2/40\n",
      "Epoch 3/40\n",
      "Epoch 4/40\n",
      "Epoch 5/40\n",
      "Epoch 6/40\n",
      "Epoch 7/40\n",
      "Epoch 8/40\n",
      "Epoch 9/40\n",
      "Epoch 10/40\n",
      "Epoch 11/40\n",
      "Epoch 12/40\n",
      "Epoch 13/40\n",
      "Epoch 14/40\n",
      "Epoch 15/40\n",
      "Epoch 16/40\n",
      "Epoch 17/40\n",
      "Epoch 18/40\n",
      "Epoch 19/40\n",
      "Epoch 20/40\n",
      "Epoch 21/40\n",
      "Epoch 22/40\n",
      "Epoch 23/40\n",
      "Epoch 24/40\n",
      "Epoch 25/40\n",
      "Epoch 26/40\n",
      "Epoch 27/40\n",
      "Epoch 28/40\n",
      "Epoch 29/40\n",
      "Epoch 30/40\n",
      "Epoch 31/40\n",
      "Epoch 32/40\n",
      "Epoch 33/40\n",
      "Epoch 34/40\n",
      "Epoch 35/40\n",
      "Epoch 36/40\n",
      "Epoch 37/40\n",
      "Epoch 38/40\n",
      "Epoch 39/40\n",
      "Epoch 40/40\n",
      "\n",
      "Best: 0.85 using {'kerasclassifier__activation': 'softmax', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 40}\n",
      "\n",
      "Means: 0.8217821766441018, Stdev: 0.03810264330050086 with: {'kerasclassifier__activation': 'softmax', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 20}\n",
      "Means: 0.8481848263504481, Stdev: 0.013442581315084483 with: {'kerasclassifier__activation': 'softmax', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 40}\n",
      "Means: 0.8448844915962849, Stdev: 0.033181904087241684 with: {'kerasclassifier__activation': 'softmax', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 60}\n",
      "Means: 0.7623762398859849, Stdev: 0.029412697115479027 with: {'kerasclassifier__activation': 'softmax', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 20}\n",
      "Means: 0.82508251867672, Stdev: 0.01387528996032988 with: {'kerasclassifier__activation': 'softmax', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 40}\n",
      "Means: 0.8217821957254567, Stdev: 0.04200846575106302 with: {'kerasclassifier__activation': 'softmax', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 60}\n",
      "Means: 0.7788778870019188, Stdev: 0.04345818999628871 with: {'kerasclassifier__activation': 'relu', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 20}\n",
      "Means: 0.834983495202395, Stdev: 0.04533236616164736 with: {'kerasclassifier__activation': 'relu', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 40}\n",
      "Means: 0.8283828374969684, Stdev: 0.04457859120560577 with: {'kerasclassifier__activation': 'relu', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 60}\n",
      "Means: 0.768976897689769, Stdev: 0.06604892077315308 with: {'kerasclassifier__activation': 'relu', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 20}\n",
      "Means: 0.7986798632656387, Stdev: 0.02219426399589959 with: {'kerasclassifier__activation': 'relu', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 40}\n",
      "Means: 0.8415841668745866, Stdev: 0.017237293069616785 with: {'kerasclassifier__activation': 'relu', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 60}\n",
      "Means: 0.8151815150043752, Stdev: 0.02409059236378785 with: {'kerasclassifier__activation': 'tanh', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 20}\n",
      "Means: 0.8481848184818482, Stdev: 0.03897267379236179 with: {'kerasclassifier__activation': 'tanh', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 40}\n",
      "Means: 0.8316831761854316, Stdev: 0.032771048541073274 with: {'kerasclassifier__activation': 'tanh', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 60}\n",
      "Means: 0.7854785606412604, Stdev: 0.026746661518703703 with: {'kerasclassifier__activation': 'tanh', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 20}\n",
      "Means: 0.801980198019802, Stdev: 0.051206404765947976 with: {'kerasclassifier__activation': 'tanh', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 40}\n",
      "Means: 0.834983486546935, Stdev: 0.0180667690376914 with: {'kerasclassifier__activation': 'tanh', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 60}\n",
      "Means: 0.7986798719210987, Stdev: 0.04049179038151545 with: {'kerasclassifier__activation': 'sigmoid', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 20}\n",
      "Means: 0.8448844837276849, Stdev: 0.04075551474718225 with: {'kerasclassifier__activation': 'sigmoid', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 40}\n",
      "Means: 0.825082514545705, Stdev: 0.037715809909867525 with: {'kerasclassifier__activation': 'sigmoid', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 60}\n",
      "Means: 0.772277223444221, Stdev: 0.09582857554267626 with: {'kerasclassifier__activation': 'sigmoid', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 20}\n",
      "Means: 0.8151815104799303, Stdev: 0.02216045047081375 with: {'kerasclassifier__activation': 'sigmoid', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 40}\n",
      "Means: 0.8415841629402866, Stdev: 0.041819959089514205 with: {'kerasclassifier__activation': 'sigmoid', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 60}\n",
      "Means: 0.8283828327758084, Stdev: 0.0408526738602068 with: {'kerasclassifier__activation': 'hard_sigmoid', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 20}\n",
      "Means: 0.8151815220861152, Stdev: 0.047228397691974115 with: {'kerasclassifier__activation': 'hard_sigmoid', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 40}\n",
      "Means: 0.8415841607764216, Stdev: 0.03524396142543215 with: {'kerasclassifier__activation': 'hard_sigmoid', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 60}\n",
      "Means: 0.7788778851331264, Stdev: 0.05556900241608819 with: {'kerasclassifier__activation': 'hard_sigmoid', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 20}\n",
      "Means: 0.8217821760539568, Stdev: 0.057315161658157504 with: {'kerasclassifier__activation': 'hard_sigmoid', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 40}\n",
      "Means: 0.834983496382685, Stdev: 0.04623831475953572 with: {'kerasclassifier__activation': 'hard_sigmoid', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 60}\n",
      "Means: 0.8118811912662519, Stdev: 0.01942966973364827 with: {'kerasclassifier__activation': 'linear', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 20}\n",
      "Means: 0.8217821797915418, Stdev: 0.043036946471539494 with: {'kerasclassifier__activation': 'linear', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 40}\n",
      "Means: 0.8184818529059784, Stdev: 0.04365445239460563 with: {'kerasclassifier__activation': 'linear', 'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 60}\n",
      "Means: 0.7887788835925238, Stdev: 0.03620389899362628 with: {'kerasclassifier__activation': 'linear', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 20}\n",
      "Means: 0.792079207527362, Stdev: 0.01808282079964959 with: {'kerasclassifier__activation': 'linear', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 40}\n",
      "Means: 0.834983494415535, Stdev: 0.04107455028240265 with: {'kerasclassifier__activation': 'linear', 'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 60}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'kerasclassifier__batch_size': [20, 40],\n",
    "             'kerasclassifier__epochs': [20, 40, 60],\n",
    "             'kerasclassifier__activation': ['softmax', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']}\n",
    "\n",
    "\n",
    "def baseline_model(activation='relu'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=inputs, activation='relu'))\n",
    "    model.add(Dense(8, activation=activation))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), KerasClassifier(build_fn=baseline_model, verbose=3))\n",
    "\n",
    "# Create grid search\n",
    "grid = GridSearchCV(estimator=pipe, param_grid=param_grid, cv=kfold, n_jobs=-1)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "# Report results\n",
    "print()\n",
    "print(\"Best: {:.2f} using {}\".format(grid_result.best_score_, grid_result.best_params_))\n",
    "print()\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning: Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sammylee/miniconda3/envs/datascience/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "Epoch 2/60\n",
      "Epoch 3/60\n",
      "Epoch 4/60\n",
      "Epoch 5/60\n",
      "Epoch 6/60\n",
      "Epoch 7/60\n",
      "Epoch 8/60\n",
      "Epoch 9/60\n",
      "Epoch 10/60\n",
      "Epoch 11/60\n",
      "Epoch 12/60\n",
      "Epoch 13/60\n",
      "Epoch 14/60\n",
      "Epoch 15/60\n",
      "Epoch 16/60\n",
      "Epoch 17/60\n",
      "Epoch 18/60\n",
      "Epoch 19/60\n",
      "Epoch 20/60\n",
      "Epoch 21/60\n",
      "Epoch 22/60\n",
      "Epoch 23/60\n",
      "Epoch 24/60\n",
      "Epoch 25/60\n",
      "Epoch 26/60\n",
      "Epoch 27/60\n",
      "Epoch 28/60\n",
      "Epoch 29/60\n",
      "Epoch 30/60\n",
      "Epoch 31/60\n",
      "Epoch 32/60\n",
      "Epoch 33/60\n",
      "Epoch 34/60\n",
      "Epoch 35/60\n",
      "Epoch 36/60\n",
      "Epoch 37/60\n",
      "Epoch 38/60\n",
      "Epoch 39/60\n",
      "Epoch 40/60\n",
      "Epoch 41/60\n",
      "Epoch 42/60\n",
      "Epoch 43/60\n",
      "Epoch 44/60\n",
      "Epoch 45/60\n",
      "Epoch 46/60\n",
      "Epoch 47/60\n",
      "Epoch 48/60\n",
      "Epoch 49/60\n",
      "Epoch 50/60\n",
      "Epoch 51/60\n",
      "Epoch 52/60\n",
      "Epoch 53/60\n",
      "Epoch 54/60\n",
      "Epoch 55/60\n",
      "Epoch 56/60\n",
      "Epoch 57/60\n",
      "Epoch 58/60\n",
      "Epoch 59/60\n",
      "Epoch 60/60\n",
      "\n",
      "Best: 0.83 using {'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 60, 'kerasclassifier__optimizer': 'adam'}\n",
      "\n",
      "Means: 0.8085808565120886, Stdev: 0.023712207545200963 with: {'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 20, 'kerasclassifier__optimizer': 'rmsprop'}\n",
      "Means: 0.8184818489716785, Stdev: 0.028761833437225222 with: {'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 20, 'kerasclassifier__optimizer': 'adam'}\n",
      "Means: 0.8184818489716785, Stdev: 0.01050562142076045 with: {'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 40, 'kerasclassifier__optimizer': 'rmsprop'}\n",
      "Means: 0.8052805335608253, Stdev: 0.020592795537008532 with: {'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 40, 'kerasclassifier__optimizer': 'adam'}\n",
      "Means: 0.8250825066771051, Stdev: 0.046311641046847166 with: {'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 60, 'kerasclassifier__optimizer': 'rmsprop'}\n",
      "Means: 0.8184818529059784, Stdev: 0.06682605585130481 with: {'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 60, 'kerasclassifier__optimizer': 'adam'}\n",
      "Means: 0.792079216379537, Stdev: 0.04688365980446376 with: {'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 20, 'kerasclassifier__optimizer': 'rmsprop'}\n",
      "Means: 0.7722772334275072, Stdev: 0.06931317412200617 with: {'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 20, 'kerasclassifier__optimizer': 'adam'}\n",
      "Means: 0.8052805268725153, Stdev: 0.02930351829131528 with: {'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 40, 'kerasclassifier__optimizer': 'rmsprop'}\n",
      "Means: 0.8052805239217903, Stdev: 0.03510003441165892 with: {'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 40, 'kerasclassifier__optimizer': 'adam'}\n",
      "Means: 0.792079211461662, Stdev: 0.059746198183734306 with: {'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 60, 'kerasclassifier__optimizer': 'rmsprop'}\n",
      "Means: 0.8250825137588451, Stdev: 0.013106632091030107 with: {'kerasclassifier__batch_size': 40, 'kerasclassifier__epochs': 60, 'kerasclassifier__optimizer': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'kerasclassifier__batch_size': [20, 40],\n",
    "             'kerasclassifier__epochs': [20, 40, 60],\n",
    "             'kerasclassifier__optimizer': ['rmsprop', 'adam']}\n",
    "\n",
    "\n",
    "def baseline_model(optimizer='adam'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=inputs, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), KerasClassifier(build_fn=baseline_model, verbose=3))\n",
    "\n",
    "# Create grid search\n",
    "grid = GridSearchCV(estimator=pipe, param_grid=param_grid, cv=kfold, n_jobs=-1)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "# Report results\n",
    "print()\n",
    "print(\"Best: {:.2f} using {}\".format(grid_result.best_score_, grid_result.best_params_))\n",
    "print()\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best GridSearchCV results came from hyperparameter tuning came form a normal baseline model (activation='relu', 'sigmoid'), {'kerasclassifier__batch_size': 20, 'kerasclassifier__epochs': 60}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with RandomForestClassifier out of curiosity: (RandomForest comes pretty close to MLP performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation scores: [0.7704918  0.86885246 0.80327869 0.78333333 0.76666667]\n",
      "Mean: 0.80, Standard Deviation: 0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sammylee/miniconda3/envs/datascience/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/sammylee/miniconda3/envs/datascience/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/sammylee/miniconda3/envs/datascience/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/sammylee/miniconda3/envs/datascience/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/sammylee/miniconda3/envs/datascience/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "results = cross_val_score(rf, X, y, cv=kfold)\n",
    "\n",
    "print(\"Cross-Validation scores: {}\".format(results))\n",
    "print(\"Mean: {:.2f}, Standard Deviation: {:.2f}\".format(results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation scores: [0.81967213 0.85245902 0.93442623 0.8        0.8       ]\n",
      "Mean: 0.84, Standard Deviation: 0.05\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, class_weight='balanced', min_samples_split=9)\n",
    "\n",
    "results = cross_val_score(rf, X, y, cv=kfold)\n",
    "\n",
    "print(\"Cross-Validation scores: {}\".format(results))\n",
    "print(\"Mean: {:.2f}, Standard Deviation: {:.2f}\".format(results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "DS43SC.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
